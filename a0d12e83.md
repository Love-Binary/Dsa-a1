# Data Structures and Algorithms - Complete Exam Notes

## Table of Contents
1. [Module 1: Introduction](#module-1-introduction)
2. [Module 2: Stacks and Queues](#module-2-stacks-and-queues)
3. [Module 3: Linked Lists and Trees](#module-3-linked-lists-and-trees)
4. [Module 4: Sorting and Hashing](#module-4-sorting-and-hashing)
5. [Module 5: Graphs](#module-5-graphs)

---

## Module 1: Introduction

### Basic Terminologies

**Elementary Data Structures:**
- Primitive data types: int, float, char, boolean
- Composite structures: arrays, records, objects
- Purpose: Organize and store data efficiently

**Data Structure Operations:**
- **Insertion**: Adding new elements into the structure
- **Deletion**: Removing elements from the structure
- **Traversal**: Visiting each element systematically
- **Searching**: Finding specific elements
- **Sorting**: Arranging elements in order

### Analysis of Algorithms

**Why Algorithm Analysis?**
- Evaluate performance and efficiency
- Compare different algorithms
- Predict behavior with large inputs
- Make informed design decisions

**Asymptotic Notations:**

**Big O (O) - Upper Bound (Worst Case)**
- Represents maximum time/space an algorithm takes
- Ignores constants and lower-order terms
- Example: O(n²) means algorithm is at most quadratic

**Omega (Ω) - Lower Bound (Best Case)**
- Represents minimum time/space required
- Describes best-case scenario
- Example: Ω(n) means algorithm requires at least linear time

**Theta (Θ) - Tight Bound (Average Case)**
- Algorithm grows exactly at this rate
- Combines both upper and lower bounds
- When Big O = Omega, we write Theta

### Common Complexity Classes (from best to worst)

1. **O(1)**: Constant time - array access, stack push/pop
2. **O(log n)**: Logarithmic - binary search, balanced tree operations
3. **O(n)**: Linear - linear search, simple iteration
4. **O(n log n)**: Linearithmic - merge sort, heap sort, efficient sorting
5. **O(n²)**: Quadratic - bubble sort, selection sort (nested loops)
6. **O(n³)**: Cubic - three nested loops
7. **O(2ⁿ)**: Exponential - recursive algorithms, subset generation
8. **O(n!)**: Factorial - permutations generation

### Time-Space Trade-off

**Concept**: Often you can optimize time at the cost of space or vice versa

**Example:**
- Store precomputed values (more space) → faster lookup (less time)
- Use minimal space → need to recompute (more time)

### Searching Algorithms

**Linear Search**
- Time Complexity: **O(n)** worst case, O(1) best case
- Works on sorted and unsorted data
- Scans from start to end
- Use when: Small dataset, unsorted data
- Algorithm: Compare each element until found or reach end

**Binary Search**
- Time Complexity: **O(log n)** for all cases
- Requires data to be SORTED
- Divides search space by 2 each iteration
- Use when: Large sorted dataset
- Algorithm:
  ```
  1. Set low = 0, high = n-1
  2. While low <= high:
     - mid = (low + high) / 2
     - If arr[mid] == target: return mid
     - If arr[mid] < target: low = mid + 1
     - Else: high = mid - 1
  3. Return -1 (not found)
  ```

**Key Comparison:**
| Feature | Linear | Binary |
|---------|--------|--------|
| Requirement | Unsorted OK | Must be sorted |
| Time Complexity | O(n) | O(log n) |
| Comparisons (n=1000) | Up to 1000 | Up to 10 |
| Space | O(1) | O(1) |
| Easier to implement | Yes | Requires more logic |

---

## Module 2: Stacks and Queues

### Stack (LIFO - Last In First Out)

**Definition**: Last element added is the first to be removed

**Real-world examples:**
- Browser back button
- Function call stack in programs
- Undo operation in editors
- Stack of plates in a cafeteria

**Stack ADT Operations:**

**Push (Insert)**
- Add element to top of stack
- Time Complexity: **O(1)**
- Check if stack is full (overflow condition)

**Pop (Delete)**
- Remove element from top of stack
- Time Complexity: **O(1)**
- Check if stack is empty (underflow condition)

**Peek/Top**
- View top element without removing
- Time Complexity: **O(1)**

**isEmpty**
- Check if stack is empty
- Time Complexity: **O(1)**

**Stack Applications:**

1. **Expression Conversion (Infix to Postfix)**
   - Infix: A + B * C (normal mathematical notation)
   - Postfix: A B C * + (operators after operands)
   - Algorithm:
     ```
     For each character in infix:
     - If operand: add to output
     - If operator: pop higher/equal precedence operators, push current
     - If '(': push to stack
     - If ')': pop until '(' found
     End: pop all remaining operators
     ```
   - Example: A + B * C → A B C * +

2. **Expression Evaluation (Postfix)**
   - Process postfix expression using stack
   - When operand: push to stack
   - When operator: pop 2 operands, apply operation, push result
   - Final result is in stack

3. **Other Applications:**
   - Function call management
   - Recursion implementation
   - Parentheses matching
   - Depth-first search (DFS)

**Stack Complexity Summary:**
- All operations: O(1) time
- Space: O(n) for n elements

---

### Queue (FIFO - First In First Out)

**Definition**: First element added is the first to be removed

**Real-world examples:**
- Ticket queue at cinema
- Printer queue
- Process scheduling in OS
- Customer service lines

**Queue ADT Operations:**

**Enqueue (Insert)**
- Add element to rear
- Time Complexity: **O(1)**

**Dequeue (Delete)**
- Remove element from front
- Time Complexity: **O(1)**

**Front/Peek**
- View first element
- Time Complexity: **O(1)**

**isEmpty/isFull**
- Check queue status
- Time Complexity: **O(1)**

### Types of Queues

**1. Simple Queue (Linear Queue)**
- Linear array implementation
- Problem: Space wastage (can't reuse front positions)
- Example: Queue of length 5, after dequeuing 3 elements, front 3 spaces are wasted

**2. Circular Queue**
- Elements arranged in circular manner
- Last position connects to first
- Solves space wastage problem of linear queue
- Implementation: rear = (rear + 1) % n
- Advantages: Efficient space utilization
- Disadvantage: More complex logic

**3. Priority Queue**
- Elements served based on priority, not insertion order
- Min-Heap: Lower priority values served first
- Max-Heap: Higher priority values served first
- Operations: O(log n) for insertion/deletion with heap
- Use case: Job scheduling, event handling

**Queue Complexity Summary:**
- All operations: O(1) time
- Space: O(n) for n elements

**Deque (Double-ended Queue)**
- Insert/delete from both ends
- Combine advantages of stack and queue
- Operations: O(1)
- Use cases: Sliding window, undo/redo operations

---

## Module 3: Linked Lists and Trees

### Linked Lists

**Definition**: Collection of nodes linked via pointers/references

**Advantages over Arrays:**
- Dynamic memory allocation
- Easy insertion/deletion (no shifting)
- No wasted memory
- No need to know size in advance

**Disadvantages:**
- Extra memory for pointers
- No random access
- More complex implementation

### Types of Linked Lists

#### 1. Singly Linked List

**Structure**: Data | Next pointer → Data | Next pointer → ... → Data | NULL

**Operations:**

**Traversal**
- Time: O(n), Space: O(1)
- Visit each node from head to end

**Searching**
- Time: O(n) worst case
- Linear search through list

**Insertion at Beginning**
- Time: O(1)
- Create node, update pointers

**Insertion at End**
- Time: O(n) (need to traverse to end)
- Traverse to last node, update pointers

**Insertion at Position**
- Time: O(n)
- Find position, update pointers

**Deletion from Beginning**
- Time: O(1)
- Update head pointer

**Deletion from End**
- Time: O(n) (need two pointers)
- Traverse to second-last node

**Deletion of Node**
- Time: O(n)
- Find node, update pointers

#### 2. Doubly Linked List

**Structure**: NULL ← [Prev | Data | Next] ↔ [Prev | Data | Next] → NULL

**Advantages:**
- Traverse both directions (forward and backward)
- Easier deletion (don't need previous pointer search)

**Disadvantages:**
- Extra memory for previous pointer
- More complex code

**Operation Complexities:**
- Traversal: O(n)
- Insertion/Deletion at beginning: O(1)
- Insertion/Deletion at end: O(1) (if tail pointer maintained)
- Insertion/Deletion at position: O(n)

#### 3. Circular Linked List

**Structure**: Nodes form a circle (last node points to first)

**Types:**
- Singly Circular: Last → First
- Doubly Circular: Bidirectional circle

**Advantages:**
- Continuous traversal without null checks
- Efficient for round-robin scheduling
- Useful for carousel/circular buffers

**Disadvantages:**
- Risk of infinite loops if not careful
- More complex logic

**Key Difference:** No explicit start/end, check for revisiting start node

### Linked List as Stack and Queue

**Stack Implementation using Linked List:**
- Push: Insert at head - O(1)
- Pop: Delete from head - O(1)
- More efficient than array (no size limitation)

**Queue Implementation using Linked List:**
- Enqueue: Insert at tail - O(1) if tail pointer maintained
- Dequeue: Delete from head - O(1)
- Advantages: Dynamic size, no space wastage

---

### Trees

**Basic Terminology:**

- **Root**: Top node
- **Leaf**: Node with no children
- **Height**: Maximum distance from root to leaf
- **Depth**: Distance of node from root
- **Subtree**: Tree formed by a node and its descendants
- **Level**: All nodes at same distance from root
- **Degree**: Number of children of a node
- **Parent/Child**: Relationship between connected nodes
- **Ancestor/Descendant**: Vertical relationships in tree

**Tree Operations:**

- **Insertion**: Add node maintaining tree properties
- **Deletion**: Remove node and reorganize
- **Searching**: Find specific node
- **Traversal**: Visit all nodes in specific order

---

#### 1. Binary Tree

**Definition**: Each node has at most 2 children (left and right)

**Properties:**
- Max nodes at level k: 2^k
- Max nodes with height h: 2^(h+1) - 1
- If n nodes, min height = log₂(n+1) - 1

**Types of Binary Trees:**

**Full Binary Tree**
- Every node has either 0 or 2 children
- No node has exactly 1 child

**Complete Binary Tree**
- All levels filled except possibly last
- Last level filled left to right

**Perfect Binary Tree**
- All internal nodes have 2 children
- All leaves at same level

**Balanced Binary Tree**
- Height difference of left and right subtrees ≤ 1

**Operation Complexities (General Binary Tree):**
- Search: O(n) worst case
- Insert: O(n) worst case
- Delete: O(n) worst case
- Traversal: O(n)

---

#### 2. Binary Search Tree (BST)

**Definition**: 
- Left child value < Parent value < Right child value
- No duplicates (or handle them specifically)

**Operations:**

**Search**
- Time: O(log n) average, **O(n) worst case** (when tree degenerates to linked list)
- Average: O(log n)
- Algorithm:
  ```
  Current = Root
  While Current != NULL:
    If value == Current.value: return Current
    If value < Current.value: Current = Current.left
    Else: Current = Current.right
  ```

**Insertion**
- Time: O(log n) average, O(n) worst case
- Find correct position maintaining BST property
- Insert as leaf node

**Deletion**
- Time: O(log n) average, O(n) worst case
- Three cases:
  1. Node is leaf: Simply remove
  2. Node has 1 child: Replace with child
  3. Node has 2 children: Replace with in-order successor (smallest in right subtree) or predecessor, then delete that node

**Traversals:**

**In-order (Left-Root-Right)**
- Output: Sorted order for BST
- Time: O(n)
- Example: BST [4,2,6,1,3,5,7] → 1,2,3,4,5,6,7

**Pre-order (Root-Left-Right)**
- Time: O(n)
- Use: Creating copy of tree

**Post-order (Left-Right-Root)**
- Time: O(n)
- Use: Deleting tree

**Level-order (BFS)**
- Time: O(n)

**Drawback**: Can degenerate to linked list in worst case (unbalanced insertion order)

---

#### 3. AVL Tree (Self-Balancing BST)

**Definition**: BST where height difference of any node's subtrees ≤ 1

**Balance Factor**: height(left) - height(right)
- Valid range: -1, 0, +1
- If outside range: rebalancing needed

**Balancing Operations (Rotations):**

**Single Right Rotation (LL case)**
- Used when left-left heavy
- Rotate right around parent

**Single Left Rotation (RR case)**
- Used when right-right heavy
- Rotate left around parent

**Left-Right Rotation (LR case)**
- Left rotation then right rotation
- Used when left-right heavy

**Right-Left Rotation (RL case)**
- Right rotation then left rotation
- Used when right-left heavy

**Operation Complexities:**
- Search: **O(log n)** guaranteed (always balanced)
- Insert: **O(log n)** (with rebalancing)
- Delete: **O(log n)** (with rebalancing)
- Height: O(log n)

**Advantages:**
- Guaranteed O(log n) for all operations
- No degeneracy

**Disadvantages:**
- Rebalancing overhead
- More complex implementation
- Not ideal if mostly searching (less rebalancing needed than frequent insertions)

---

#### 4. Threaded Binary Tree

**Definition**: Nodes contain additional pointers (threads) for in-order traversal

**Purpose**: 
- Avoid recursion/stack in traversal
- Use NULL pointers efficiently
- Direct predecessor/successor access

**Types:**
- Single threaded: In-order successor
- Doubly threaded: Both successor and predecessor

**Advantages:**
- Efficient traversal without stack
- Space utilization of NULL pointers

---

#### 5. B-Tree and B+ Tree

**B-Tree Properties:**
- Multi-way tree (more than 2 children per node)
- Balanced - all leaf nodes at same level
- Useful for database indexing
- Order m: Each node has at most m children

**B+ Tree:**
- Extension of B-tree
- All data in leaf nodes
- Internal nodes store only keys for routing
- Leaves linked for range queries
- Better for range searches and database operations

**Operation Complexities:**
- Search/Insert/Delete: O(log n)
- Highly efficient for disk access

---

## Module 4: Sorting and Hashing

### Sorting Algorithms

#### 1. Bubble Sort (Simple Comparison Sort)

**How it works:**
- Compare adjacent pairs
- Swap if they're in wrong order
- Repeat until no swaps needed
- Largest element "bubbles" to end each pass

**Time Complexity:**
- Best case: **O(n)** - already sorted (with optimization)
- Average case: **O(n²)**
- Worst case: **O(n²)** - reverse sorted

**Space Complexity:** O(1)

**Properties:**
- In-place algorithm
- Stable (equal elements maintain order)
- Simple to implement
- Very slow for large datasets

**Optimization:** Stop if no swaps in a pass

---

#### 2. Selection Sort

**How it works:**
- Find minimum element, place at beginning
- Find next minimum from remaining, place next
- Repeat until sorted

**Time Complexity:**
- Best, Average, Worst: **O(n²)**
- Always O(n²), no best case optimization

**Space Complexity:** O(1)

**Properties:**
- In-place
- Not stable
- Few swaps (at most n-1)
- Predictable performance

---

#### 3. Insertion Sort

**How it works:**
- Build sorted array one element at a time
- Compare element with sorted portion
- Shift larger elements right
- Insert in correct position

**Time Complexity:**
- Best case: **O(n)** - already sorted
- Average case: **O(n²)**
- Worst case: **O(n²)** - reverse sorted

**Space Complexity:** O(1)

**Properties:**
- In-place
- Stable
- Good for small arrays or nearly sorted data
- Adaptive (efficient for sorted data)

---

#### 4. Quick Sort (Divide and Conquer)

**How it works:**
1. Choose pivot element
2. Partition: elements < pivot on left, > pivot on right
3. Recursively sort left and right partitions
4. Combine (already in place)

**Time Complexity:**
- Best case: **O(n log n)** - balanced partitions
- Average case: **O(n log n)**
- Worst case: **O(n²)** - pivot always at end

**Space Complexity:** O(log n) - recursion stack

**Properties:**
- In-place
- Not stable
- Very fast in practice
- Cache-friendly
- Random pivot selection improves average case

**Partitioning Strategies:**
- First element as pivot
- Last element as pivot
- Random element as pivot
- Median-of-three

---

#### 5. Merge Sort (Divide and Conquer)

**How it works:**
1. Divide array into halves
2. Recursively sort both halves
3. Merge two sorted halves into one sorted array

**Time Complexity:**
- Best, Average, Worst: **O(n log n)** - guaranteed
- Always balanced divisions

**Space Complexity:** O(n) - temporary arrays

**Properties:**
- Not in-place (needs extra space)
- Stable
- Predictable O(n log n) performance
- Good for linked lists (no random access needed)

**Advantages:**
- Guaranteed O(n log n)
- Stable sort
- Good for external sorting

**Disadvantages:**
- Extra O(n) space
- Slower for small arrays due to overhead

---

#### 6. Heap Sort

**How it works:**
1. Build max-heap from array
2. Swap root (max) with last element
3. Reduce heap size, heapify
4. Repeat until sorted

**Time Complexity:**
- Best, Average, Worst: **O(n log n)**
- Guaranteed O(n log n) with no extra space

**Space Complexity:** O(1) - in-place

**Properties:**
- In-place
- Not stable
- Good for memory-constrained systems

**Heapify Process:**
- Sift down: Compare with children, swap with larger, continue
- Building heap: O(n)
- Each extraction: O(log n)

---

### Sorting Algorithm Comparison

| Algorithm | Best | Average | Worst | Space | Stable | In-place |
|-----------|------|---------|-------|-------|--------|----------|
| Bubble | O(n) | O(n²) | O(n²) | O(1) | Yes | Yes |
| Selection | O(n²) | O(n²) | O(n²) | O(1) | No | Yes |
| Insertion | O(n) | O(n²) | O(n²) | O(1) | Yes | Yes |
| Quick | O(n log n) | O(n log n) | O(n²) | O(log n) | No | Yes |
| Merge | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes | No |
| Heap | O(n log n) | O(n log n) | O(n log n) | O(1) | No | Yes |

**When to use which:**
- Small arrays: Insertion sort
- Nearly sorted: Insertion sort
- Need stability: Merge sort
- Memory constraint: Heap sort or Quick sort
- Average performance: Quick sort
- Guaranteed: Merge sort or Heap sort
- Simple code: Bubble/Selection (educational)

---

### Hashing

**Hash Function**: Maps key → array index

**Hash Table**: Array that stores key-value pairs using hash function

**Goals:**
- O(1) average insertion, deletion, search
- Minimize collisions
- Uniform distribution

### Collision Resolution Techniques

**When collision occurs**: Two keys hash to same index

#### 1. Separate Chaining

**Method**: Each index contains linked list of all colliding elements

**Implementation:**
- hash_table[index] = linked list of (key, value) pairs
- Multiple elements can hash to same index

**Operations:**
- Search: Hash key, traverse list at that index - Average O(1), Worst O(n)
- Insert: Hash key, add to list at index - O(1) average
- Delete: Hash key, search and remove from list - O(1) average, O(n) worst

**Load Factor**: λ = n/m (n=elements, m=table size)
- Average chain length = λ
- Good when λ < 1

**Advantages:**
- Simple to implement
- Handles large collisions well
- Dynamic size (lists grow as needed)

**Disadvantages:**
- Extra memory for pointers
- Poor cache locality
- Potential long chains

#### 2. Open Addressing (Closed Hashing)

**Method**: Find alternative empty slot using probing

**Linear Probing**
- Probe next consecutive slot: (h + i) mod m
- Time: O(1) average, O(n) worst
- Problem: Primary clustering
- When key not found at hash position, check next, next, next...

**Quadratic Probing**
- Probe using quadratic offsets: (h + i²) mod m
- Reduces primary clustering
- Better distribution
- Secondary clustering still possible

**Double Hashing**
- Use second hash function: (h₁(k) + i * h₂(k)) mod m
- Best distribution
- Requires two good hash functions

**Advantages:**
- No extra space for pointers
- Better cache locality
- All data in single array

**Disadvantages:**
- Clustering (probes form clusters)
- Deletion is complex (need lazy deletion/tombstone)
- Fixed table size
- Load factor must be < 1

**Rehashing**: When table gets too full (λ > 0.75)
- Create larger table (usually 2x)
- Rehash all elements
- Time: O(n) amortized

#### 3. Other Techniques

**Perfect Hashing**
- Maps all keys to different positions
- No collisions
- Requires more space
- Used when keys are known in advance

**Coalesced Hashing**
- Hybrid of chaining and open addressing
- Reduces space overhead

**Cuckoo Hashing**
- Uses two hash functions
- Guaranteed O(1) lookup
- Variable insertion time

### Hashing Complexity Summary

**Average Case:**
- Search: **O(1)**
- Insert: **O(1)**
- Delete: **O(1)**

**Worst Case:**
- All operations: **O(n)** (all collisions)

**Load Factor Considerations:**
- Keep 0.5 < λ < 0.75 for good performance
- Monitor and rehash when needed

---

## Module 5: Graphs

### Basic Terminology

**Graph**: Collection of vertices connected by edges

**Vertices (Nodes)**: Points in graph

**Edges**: Connections between vertices

**Types of Graphs:**

**Directed Graph (Digraph)**
- Edges have direction (u → v)
- Edge from u to v doesn't mean edge from v to u
- Example: Web pages, dependency graphs

**Undirected Graph**
- Edges have no direction
- Edge between u and v implies edge between v and u
- Example: Social networks, road networks

**Weighted Graph**
- Edges have weights/costs
- Example: Road distances, flight prices

**Unweighted Graph**
- All edges have equal weight (usually 1)

### Graph Representations

#### Adjacency Matrix

**Structure**: 2D array where A[i][j] represents edge from vertex i to j

**For Undirected**: A[i][j] = A[j][i]
**For Weighted**: Store weight instead of 1

**Advantages:**
- O(1) edge lookup
- Simple operations
- Dense graphs efficient

**Disadvantages:**
- O(V²) space (wasteful for sparse graphs)
- Slow to find all neighbors

#### Adjacency List

**Structure**: Array of lists, where list i contains all neighbors of vertex i

**Advantages:**
- O(V + E) space (efficient for sparse graphs)
- Easy to find neighbors
- Better for most algorithms

**Disadvantages:**
- O(E/V) average time to check edge existence
- More complex implementation

### Graph Traversal Algorithms

#### 1. Breadth-First Search (BFS)

**Concept**: Explore vertices level by level, visiting all neighbors before moving deeper

**Data Structure**: Queue (FIFO)

**Algorithm:**
```
1. Start from source vertex
2. Mark as visited, enqueue
3. While queue not empty:
   - Dequeue vertex
   - For each unvisited neighbor:
     - Mark visited
     - Enqueue
4. Continue until all reachable vertices visited
```

**Time Complexity:** **O(V + E)**
- Visit each vertex once: V
- Check each edge once: E

**Space Complexity:** O(V) - Queue and visited array

**Use Cases:**
- Shortest path in unweighted graphs
- Level-order traversal
- Connected components
- Bipartite checking

#### 2. Depth-First Search (DFS)

**Concept**: Explore as deep as possible before backtracking

**Data Structure**: Stack (LIFO) - often done recursively

**Algorithm:**
```
1. Mark current vertex as visited
2. For each unvisited neighbor:
   - Recursively call DFS
3. Backtrack
```

**Time Complexity:** **O(V + E)**
- Visit each vertex once: V
- Check each edge once: E

**Space Complexity:** O(V) - Recursion stack

**Use Cases:**
- Topological sorting
- Cycle detection
- Strongly connected components
- Path finding

#### 3. Comparison: BFS vs DFS

| Aspect | BFS | DFS |
|--------|-----|-----|
| Data Structure | Queue | Stack/Recursion |
| Order | Level by level | Depth first |
| Time | O(V+E) | O(V+E) |
| Space | O(V) | O(V) |
| Shortest Path (unweighted) | Yes | No |
| All Paths | No | Yes |
| Memory Usage | High for wide graphs | High for deep graphs |

### Graph Operations and Applications

**Connectivity:** Check if path exists between two vertices
- BFS/DFS: O(V+E)

**Connected Components:** Find all connected subgraphs
- DFS/BFS: O(V+E)

**Cycle Detection:**
- Undirected: DFS with visited parent check
- Directed: DFS with recursion stack

**Topological Sort:** Linear order respecting directed edges
- DFS-based: O(V+E)
- Kahn's algorithm (BFS-based): O(V+E)
- Use: Task scheduling, dependency resolution

**Shortest Path:** Find minimum cost path
- Unweighted: BFS - O(V+E)
- Weighted positive: Dijkstra - O((V+E) log V)
- Negative weights: Bellman-Ford - O(VE)
- All pairs: Floyd-Warshall - O(V³)

---

## Important Formulas and Concepts

### Recursion and Complexity

**Recurrence Relation**: Mathematical equation describing recursive algorithm

**Master Theorem**: For T(n) = aT(n/b) + f(n):
1. If f(n) = O(n^(log_b(a)-ε)): T(n) = O(n^log_b(a))
2. If f(n) = Θ(n^log_b(a)): T(n) = Θ(n^log_b(a) * log n)
3. If f(n) = Ω(n^(log_b(a)+ε)): T(n) = Θ(f(n))

### Key Insights for Exam

1. **Time vs Space**: Often inversely related
2. **Sorting**: Know when each algorithm is best
3. **Trees**: Balance ensures O(log n)
4. **Hashing**: O(1) average, O(n) worst
5. **Graphs**: BFS/DFS are fundamental
6. **Complexity**: Always consider best, average, worst cases
7. **Stability**: Important for multi-key sorting
8. **In-place**: Important for space-constrained systems

---

## Quick Reference: Algorithm Selection

**Need to Search?**
- Unsorted data: Linear search O(n)
- Sorted data: Binary search O(log n)
- Need many searches: Hash table O(1) average

**Need to Sort?**
- Small array: Insertion sort
- Need stability: Merge sort
- Best average: Quick sort
- Guaranteed best: Heap sort

**Need a Queue?**
- FIFO behavior: Queue (Simple/Circular/Priority)
- LIFO behavior: Stack

**Need to Find Path?**
- Unweighted: BFS
- Weighted: Dijkstra
- Need all paths: DFS

**Need to Store Key-Value?**
- Few collisions: Open addressing
- Many collisions: Separate chaining
- Need perfect hash: Perfect hashing (if size known)

---

## Practice Tips

1. **Code each algorithm**: Understanding implementation helps recognition
2. **Trace examples**: Work through small examples by hand
3. **Complexity analysis**: Practice counting operations
4. **Compare algorithms**: Know pros/cons of each
5. **Real-world applications**: Understand when to use what
6. **Edge cases**: Empty data, single element, already sorted/reverse sorted
7. **Time management**: In exam, don't spend too much time on one question

---

**Good luck with your exam! Remember to show all steps in complexity analysis and explain your reasoning clearly.**
